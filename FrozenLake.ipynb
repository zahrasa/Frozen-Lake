{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd4GU-Pf4vd9"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b8Ie-mkhiGjT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from gym import Env\n",
    "import datetime\n",
    "\n",
    "class FrozenLake(Env):\n",
    "    def __init__(self,random_num:int=256, nonStationary = False):\n",
    "        self.random_num = random_num\n",
    "        self.nonStationary = nonStationary\n",
    "        \n",
    "        np.random.seed(self.random_num)\n",
    "        self.beginMap = make_map(self.random_num) #*2\n",
    "        self.beginMap[self.beginMap>1] = 1\n",
    "        self.endMap = make_map(self.random_num + 100)\n",
    "        \n",
    "        self.changeDir = self.endMap - self.beginMap\n",
    "        self.changeDir *= 1/11000\n",
    "\n",
    "        self.fixedMap = self.beginMap\n",
    "\n",
    "        np.random.seed(datetime.datetime.now().microsecond)\n",
    "        \n",
    "        self.map = copy.deepcopy(self.fixedMap)\n",
    "        self.time = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.NSreset()\n",
    "        if not self.nonStationary:\n",
    "            self.map = copy.deepcopy(self.fixedMap)\n",
    "            self.time = 0\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def NSreset(self):\n",
    "        self.time += 1\n",
    "        self.map += self.changeDir\n",
    "\n",
    "        self.map[self.map>0.95]=0.95\n",
    "        self.map[self.map<0.0]=0.0\n",
    "\n",
    "        self.state = (0,0)\n",
    "        self.done = False\n",
    "        return self.state\n",
    "    \n",
    "    def states_transitions(self, state, action):\n",
    "        x = state[0]\n",
    "        y = state[1]\n",
    "        states = np.array([[x,y-1], [x,y+1], [x-1 ,y], [x+1,y] ])\n",
    "\n",
    "\n",
    "        if action == UP:\n",
    "            selected = states[2]\n",
    "        if action == DOWN:\n",
    "            selected = states[3]\n",
    "        if action == RIGHT:\n",
    "            selected = states[1]\n",
    "        if action == LEFT:\n",
    "            selected = states[0]\n",
    "\n",
    "        zero = np.zeros((4,2)).astype(int)\n",
    "        three = (3 * np.ones((4,2))).astype(int)\n",
    "        output = np.maximum(np.minimum(states, three),zero)\n",
    "        output, indices = np.unique(output, axis = 0, return_counts= True)\n",
    "\n",
    "        \n",
    "        selected = np.maximum(np.minimum(selected, three[0]), zero[0])\n",
    "        probs = indices * 0.025\n",
    "        probs[np.argmax(np.sum(selected == output, axis = 1))] += 0.9\n",
    "\n",
    "        return list(zip(output[:,0],output[:,1])), probs\n",
    "    \n",
    "    def possible_consequences(self,action:int,state_now=None):\n",
    "\n",
    "        if state_now==None:\n",
    "            state_now = self.state\n",
    "\n",
    "        state = [state_now[0],state_now[1]]\n",
    "        states, probs = self.states_transitions(state, action)\n",
    "        aa = np.array(states) \n",
    "        fail_probs = self.map[(aa[:,0]),(aa[:,1])]\n",
    "        dones = np.sum(aa == 3, axis = 1) == 2\n",
    "        return states, probs, fail_probs,dones\n",
    "    \n",
    "    def step(self, a:int):\n",
    "        if not (a in range(4)):\n",
    "            raise Exception(\"action is not available!!!\")\n",
    "        \n",
    "        states, probs, fail_probs,dones = self.possible_consequences(a)\n",
    "        \n",
    "        next_idx = np.random.choice(np.arange(len(states)), p = probs)\n",
    "        next_state = states[next_idx]\n",
    "        self.state = tuple(next_state)\n",
    "        \n",
    "        self.done = dones[next_idx]\n",
    "\n",
    "        r = -1\n",
    "\n",
    "        if self.done:\n",
    "            r += 60\n",
    "        elif np.random.rand()< fail_probs[next_idx]:\n",
    "            r -= 15\n",
    "            self.done = True\n",
    "\n",
    "        return (self.state, r, self.done, {})\n",
    "\n",
    "    def render(self,state=None):\n",
    "        if state == None:\n",
    "            state = self.state\n",
    "\n",
    "        out = \"\"\n",
    "        for i in range(4):\n",
    "            out += \"\\n------------------------------\\n| \"\n",
    "            for j in range(4):\n",
    "                if (i,j) == state:\n",
    "                    out += \"\\033[44m{:.3f}\\033[0m | \".format(self.map[i,j])\n",
    "                else :\n",
    "                    out += \"{:.3f} | \".format(self.map[i,j])\n",
    "\n",
    "        out += \"\\n------------------------------\"\n",
    "        print(out)\n",
    "\n",
    "    def environment_states(self):\n",
    "        env_states = []\n",
    "        for state_index in range(16):\n",
    "            s0 = state_index % 4\n",
    "            s1 = state_index//4\n",
    "            env_states.append((s0,s1))\n",
    "        return env_states\n",
    "\n",
    "        \n",
    "def set_max_min(var,maximum,minimum):\n",
    "    return min(max(var,minimum),maximum)\n",
    "\n",
    "def make_map(random_num):\n",
    "    np.random.seed(random_num)  \n",
    "    move = np.zeros(6)\n",
    "    idx = np.random.choice(range(6),size=3,replace=False)\n",
    "    move[idx] = 1\n",
    "\n",
    "    point = [0,0]\n",
    "    lowprobs = [tuple(point)]\n",
    "\n",
    "    for m in move:\n",
    "        if m:\n",
    "            point[0] += 1\n",
    "        else:\n",
    "            point[1] += 1\n",
    "        lowprobs.append(tuple(point))\n",
    "    \n",
    "    map = np.random.rand(4,4)\n",
    "    idx = np.array(lowprobs)\n",
    "\n",
    "    map[idx[:,0],idx[:,1]] = 0.001 \n",
    "    map[0,0] = 0.0\n",
    "    map[3,3] = 0.0 \n",
    "\n",
    "    return map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba-NtxEn5LJ0"
   },
   "source": [
    "# HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "54H4VswF5Kot"
   },
   "outputs": [],
   "source": [
    "#%% allowed actions\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "ACTIONS = [LEFT,DOWN,RIGHT,UP]\n",
    "\n",
    "#%% hyperparameters\n",
    "EPISODES = 10000\n",
    "EPSILON = 0.1\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mdub8jub5bM9"
   },
   "source": [
    "## Map of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JvWeNrBx4or9"
   },
   "outputs": [],
   "source": [
    "RAND_NUM = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ME5gllo7g0p7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with fail probabilities :\n",
      "\n",
      "------------------------------\n",
      "| \u001b[44m0.000\u001b[0m | 0.179 | 0.053 | 0.079 | \n",
      "------------------------------\n",
      "| 0.001 | 0.001 | 0.441 | 0.774 | \n",
      "------------------------------\n",
      "| 0.302 | 0.001 | 0.879 | 0.328 | \n",
      "------------------------------\n",
      "| 0.138 | 0.001 | 0.001 | 0.000 | \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "environment = FrozenLake(random_num=RAND_NUM)\n",
    "\n",
    "print(\"Environment with fail probabilities :\")\n",
    "environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z-KEpaeOcAh"
   },
   "source": [
    "## <h2><font color=indigo> Agent Implementation\n",
    "Implement your q-learning (off-policy TD) agent here. You need to utilize the step function provided in the Environment class to interact with frozen lake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9P-5IZqIeco8"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "class Q_Learning:\n",
    "    def __init__(self, id, environment, discount , learning_rate = 0.1 , epsilon = 0.1 ,episodes=10000):\n",
    "\n",
    "        self.environment = environment\n",
    "        self.discount = discount\n",
    "        self.episodes = episodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.environment = environment\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = 4\n",
    "    \n",
    "    def qLearning(self):\n",
    "        # List of rewards\n",
    "        rewards = []\n",
    "        max_epsilon = 1.0             # Exploration probability at start\n",
    "        min_epsilon = 0.01            # Minimum exploration probability \n",
    "        decay_rate = 0.005            # Epsilon decay rate\n",
    "        \n",
    "        # create q table 16 in 4\n",
    "        qtable = np.zeros((len(environment.environment_states()), self.n_actions))\n",
    "\n",
    "\n",
    "        # For life or until learning is stopped\n",
    "        for episode in range(self.episodes):\n",
    "            # Reset the environment\n",
    "            state = 0\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "            max_step = 300\n",
    "\n",
    "            for step in range(max_step):\n",
    "                # Choose an action a in the current world state (s)\n",
    "                # First we randomize a number\n",
    "                exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "                # If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "                if exp_tradeoff > self.epsilon:\n",
    "                    action = np.argmax(qtable[state,:])\n",
    "                  \n",
    "                # Else doing a random choice for action(0 to 3) --> exploration\n",
    "                else:\n",
    "                    action = random.randint(0, self.n_actions-1)\n",
    "           \n",
    "\n",
    "                # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, done, info = self.environment.step(action)\n",
    "                  \n",
    "                    \n",
    "                # convert state(i,j) to state k\n",
    "                # k is 0 to 15\n",
    "                k = -1   \n",
    "                while k < 16:\n",
    "                    for i in range(4):\n",
    "                        for j in range(4):\n",
    "                            k = k + 1\n",
    "                            if new_state == (i,j):\n",
    "                                new_state = k               \n",
    "                    \n",
    "                    \n",
    "                # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "                # qtable[new_state,:] : all the actions we can take from new state\n",
    "                qtable[state, action] = qtable[state, action] + self.learning_rate * (reward + self.discount * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "\n",
    "                total_rewards += reward\n",
    "\n",
    "                # Our new state is state\n",
    "                state = new_state\n",
    "\n",
    "                # If done (if we're dead) : finish episode\n",
    "                if done == True: \n",
    "                    break\n",
    "\n",
    "            # Reduce epsilon (because we need less and less exploration)\n",
    "            epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "            rewards.append(total_rewards)\n",
    "\n",
    "            \n",
    "        print(\"Q table is:\\n\\n\", qtable)\n",
    "        print (\"\\nScore over time: \" +  str(sum(rewards)/self.episodes))\n",
    "        \n",
    "        \n",
    "        policy = []\n",
    "        for state in range(16):  \n",
    "            action = np.argmax(qtable[state,:])\n",
    "            if action == 0:\n",
    "                policy.append('LEFT')\n",
    "            if action == 1:\n",
    "                policy.append('DOWN')\n",
    "            if action == 2:\n",
    "                policy.append('RIGHT')\n",
    "            if action == 3:\n",
    "                policy.append('UP')\n",
    "                \n",
    "\n",
    "        return qtable,policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6J4uXQGuTgs"
   },
   "source": [
    "## <h2><font color=indigo> Q Values\n",
    "Return the Q values that your agent learns in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WYZfiWY6qMch",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q table is:\n",
      "\n",
      " [[ 37.63600323  49.50180441  30.2027829   37.1977268 ]\n",
      " [ 23.34894751  10.91455897  -1.6918125   -1.61965082]\n",
      " [  5.65388265  -8.87        -5.035625    -1.8549375 ]\n",
      " [-13.39803125 -14.2875     -17.62142748 -17.27408109]\n",
      " [  1.67394865  -1.28573195  16.68799591  10.00324472]\n",
      " [ -1.34792828  37.34677353  14.52889747  -1.4825    ]\n",
      " [ 28.10131613  -8.          -8.25        -0.975     ]\n",
      " [ 16.40130057  37.91685756  -8.          -4.6664375 ]\n",
      " [ -8.          -1.0875      -1.15962969   8.94468287]\n",
      " [ -1.70323321  45.05354138  17.30236464  14.66798277]\n",
      " [ 32.65298904  47.77168971   0.           0.        ]\n",
      " [ 28.70350642  38.32810555  32.78340658  15.95327612]\n",
      " [ -0.975       -8.          24.75022394  -0.93204172]\n",
      " [ -1.38125     16.86419735  51.92113822  35.23543774]\n",
      " [ 40.32527456  47.02678837  55.28093077  27.99798611]\n",
      " [  0.           0.           0.           0.        ]]\n",
      "\n",
      "Score over time: 55.6269\n"
     ]
    }
   ],
   "source": [
    "agent = Q_Learning('zahrasadat sajjadi', environment, 0.9 , learning_rate = 0.5 , epsilon = 0.1 ,episodes= 10000)\n",
    "Q , policy = agent.qLearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMojYRGVvAXk"
   },
   "source": [
    "## <h2><font color=darkcyan> Policy\n",
    "Return the optimal policy that your agent learns in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9EFY3T0r9OHW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOWN',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'LEFT',\n",
       " 'RIGHT',\n",
       " 'DOWN',\n",
       " 'LEFT',\n",
       " 'DOWN',\n",
       " 'UP',\n",
       " 'DOWN',\n",
       " 'DOWN',\n",
       " 'DOWN',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'RIGHT',\n",
       " 'LEFT']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DOWN LEFT LEFT LEFT \n",
      " RIGHT DOWN LEFT DOWN \n",
      " UP DOWN DOWN DOWN \n",
      " RIGHT RIGHT RIGHT LEFT "
     ]
    }
   ],
   "source": [
    "# policy in table shape\n",
    "for i in range(len(policy)):\n",
    "    if i % 4!=0:\n",
    "        print(policy[i],'',end = '')\n",
    "        \n",
    "    else:\n",
    "        print('\\n',policy[i],'',end = '')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "StudentName__HW5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
